=============================================================
Level 4: 運用編
=============================================================

目的・ゴール: ラボを実施する環境の確認
=============================================================

本番運用になった場合に必要となるアプリケーションの可用性、インフラの可用性、非機能要件の実現について検討します。

Level3まででアプリケーションを迅速にリリースする仕組みを作成しました。ここからはマスタのHA化やバックアップをどうするかを検討します。


流れ
=============================================================


本レベルでは運用時に課題となり、解決が必要となる項目についてリストしました。
現在の環境に対して変更をして見てください位。ここでは実際に手をうごかしてもらってもいいですし、
ディスカッションにしていただいても構いません。


アプリケーションの可用性を向上させる
=============================================================

アプリケーションの可用性を挙げるためWorkload APIを使用する
-------------------------------------------------------------

* ReplicaSet
* DaemonSet
* StatefulSet

リリース後に問題発生、データをすべて戻す
-------------------------------------------------------------

アプリケーションはDeploymentでリビジョン管理します。

* アプリケーションはrollout機能ですぐに再デプロイ可能
* データの戻しについても考慮すること

* Deployment

    * rollout

ローリングアップデート
-------------------------------------------------------------

kubectl set image deployment/DEPLOYMENT CONTAINER=IMAGE_NAME:TAG

アプリケーション負荷に応じたスケールアウト・イン
-------------------------------------------------------------

* Horizontal Pod Autoscaler

インフラの可用性を向上させる
=============================================================


ログの確認、デバッグ方法
-------------------------------------------------------------

標準のkubectlだとログがおいづらいときがあるため以下のツールの検討もする。

* kubernetesホストの/var/log/containerにログは保管。(systemd系の場合）
* sternなどのログ管理ツールを活用する
* fluetndを使いログを集約する。

コンテナクラスタの監視
-------------------------------------------------------------

監視の方針として

* 標準の heapster + Grafana + InfluxDB
* Prometheus + Grafana

    * Helm で入れる
    *  helm install stable/prometheus

* k8s Master の冗長化

* Metrics = Prometeus, Traces = Zipkin, ServiceGraph = Graphbiz & Prometeus


バックアップはどうするか？
-------------------------------------------------------------

* etcd のバックアップ戦略
* コンテナ化されたアプリケーションの永続化データ
* コンテナイメージ

セキュリティアップグレード
-------------------------------------------------------------

例えば、脆弱性があった場合の対処方法はどうすればよいか。

* ノードごとにバージョンアップするため、ある程度の余力を見込んだ設計とする。
* kubectl drain を使いノードで動いているPodを別ノードで起動、対象ノードをアップデートし戻す。

DRをどうするか？
-------------------------------------------------------------

別クラスタで作成されたPVはそのままは参照できないので以下の方法を検討する。

* Cluster federation
* CSI (Container Storage Interface)の既存ボリュームのインポートに対応をまつ
* Heptio ark: git clone git@github.com:heptio/ark.git, https://github.com/heptio/ark + SnapMirror


LoadBalancer の可用性を向上させる
-------------------------------------------------------------

外部からのアクセスをうけつけるため、Ingressを使うケースがあるが本番運用を想定した場合の構成方法。


